{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOFzt5AjEnLKfETWa95D4c/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UWGJcan23KG2","executionInfo":{"status":"ok","timestamp":1719427653813,"user_tz":300,"elapsed":36006,"user":{"displayName":"Sasan Azizian","userId":"17610892618478316694"}},"outputId":"aa0d8568-b9da-4744-ad1c-3dad6821cf9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","All_proteins\t       mirna_model.h.May15\t model_with_RBP_miRNA.ipynb  split_files2.zip\n","destination.zip        mirna_model.h.May18\t model_with_RBP_pssm.ipynb   Test\n","githup\t\t       mirna_model.h.May18.TEST  PSSM-Data\t\t     TEST-Similarity.ipynb\n","miRNA-DATA\t       mirna_model.h.May20\t RBP-Data\t\t     Untitled1.ipynb\n","mirna_model.h2024-185  mirna_model.h.May21\t RBP-Data-Sample\n","mirna_model.h2024-old  mirna_model.h.TEST\t rna_model.h1\n","mirna_model.h.Jun08    mirna_model.N2024\t rna_model.h5\n","Number of files in /content/drive/My Drive/Data_Final_Code/PSSM-Data: 498\n"]}],"source":["# Essential imports\n","import numpy as np\n","import pandas as pd\n","import os\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","# TensorFlow and Keras\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Dropout, Bidirectional, Concatenate, Layer\n","from tensorflow.keras.callbacks import Callback\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.optimizers import Adam\n","import tensorflow.keras.backend as K\n","from sklearn.preprocessing import MinMaxScaler\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.models import Model,load_model,save_model\n","from keras import layers\n","from tensorflow.keras.layers import Layer\n","\n","\n","# Drive mounting\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","\n","\n","files = !ls \"/content/drive/My Drive/Data_Final_Code\"\n","for file in files:\n","  print(file)\n","\n","pssm_path = '/content/drive/My Drive/Data_Final_Code/PSSM-Data'\n","RBP_path = '/content/drive/My Drive/Data_Final_Code/RBP-Data/'\n","# RBP_path = '/content/drive/My Drive/Data_Final_Code/RBP-Data-Sample/'\n","\n","\n","\n","miRNA_path = '/content/drive/My Drive/Data_Final_Code/miRNA-DATA/clean_miRNA_no_duplicates.txt'\n","# miRNA_path = '/content/drive/My Drive/Data_Final_Code/miRNA-DATA/miR-223.txt'\n","# miRNA_path ='/content/drive/My Drive/Data_Final_Code/miRNA-DATA/clean_miRNA_no_duplicates_rna.txt'\n","# miRNA_path ='/content/drive/My Drive/Data_Final_Code/miRNA-DATA/reversed_miRNA_sequences.txt'\n","\n","num_files = len(os.listdir(pssm_path))\n","\n","print(f\"Number of files in {pssm_path}: {num_files}\")\n","pssm_path = '/content/drive/My Drive/Data_Final_Code/PSSM-Data'\n","RBP_path = '/content/drive/My Drive/Data_Final_Code/RBP-Data/'\n","# RBP_path = '/content/drive/My Drive/Data_Final_Code/RBP-Data-Sample/'\n","# miRNA_path = '/content/drive/My Drive/Data_Final_Code/miRNA-DATA/clean_miRNA_no_duplicates.txt'\n","miRNA_path = '/content/drive/My Drive/Data_Final_Code/miRNA-DATA/miR-223.txt'\n","\n","\n","#Read and Process RBP"]},{"cell_type":"code","source":["#Read and Process RBP\n","\n","def process_sequences(sequences):\n","    if not sequences:  # Check if the sequences list is empty\n","          return np.array([]), set()  # Return an empty array and set if no sequences to process\n","    unique_characters = set(''.join(sequences))\n","    letter2number = {l: i for i, l in enumerate(unique_characters, start=1)}\n","    processed_seqs = [[letter2number[char] for char in seq] for seq in sequences]\n","    return pad_sequences(processed_seqs, padding='post'), unique_characters\n","\n","def clean_data(sequences, labels):\n","    cleaned_sequences = []\n","    cleaned_labels = []\n","    for seq, label in zip(sequences, labels):\n","        if pd.isna(seq) or pd.isna(label):\n","            # print(\"Warning: Missing sequence or label\")\n","            continue\n","        try:\n","            cleaned_label = int(float(label))\n","            if cleaned_label in [0, 1]:\n","                cleaned_sequences.append(seq)\n","                cleaned_labels.append(cleaned_label)\n","            else:\n","                print(f\"Warning: Label not 0 or 1 encountered: '{label}'\")\n","        except ValueError:\n","            print(f\"Warning: Non-numeric label encountered: '{label}'\")\n","    return cleaned_sequences, cleaned_labels\n","\n","\n","def read_and_process_data(file_path):\n","    data = pd.read_csv(file_path, sep='\\s+', names=['sequence', 'label'])\n","    sequences, labels = clean_data(data['sequence'].tolist(), data['label'].tolist())\n","    processed_sequences, unique_characters = process_sequences(sequences)\n","    labels = np.array(labels).astype(np.float32)\n","    return processed_sequences, labels, unique_characters\n","\n","pssm_data_pca_dict = {\n","    \"SampleProtein1\": [0.1, 0.2, 0.3],\n","    \"SampleProtein2\": [0.4, 0.5, 0.6]\n","}\n","def process_all_files(directory_path, pssm_data_pca_dict):\n","    data_dict = {}\n","    data_dict_nopssm = {}\n","    global_unique_characters = set()\n","    added_items_count = 0\n","    added_items_nopssm_count = 0  # Counter for items added to data_dict_nopssm\n","\n","    for file_name in os.listdir(directory_path):\n","        if file_name.endswith('.fa'):\n","            rbp_name = file_name.split('.')[0]\n","            file_path = os.path.join(directory_path, file_name)\n","            sequences, labels, unique_characters = read_and_process_data(file_path)\n","\n","            # Attempt to find an exact or similar key\n","            # key_to_use   = rbp_name if rbp_name in pssm_data_pca_dict else find_similar_key(rbp_name, pssm_data_pca_dict.keys())\n","            key_to_use = False\n","\n","            if key_to_use:\n","                data_dict[rbp_name] = {\n","                    'sequences': sequences,\n","                    'pssm_data': pssm_data_pca_dict[key_to_use],\n","                    'labels': labels\n","                }\n","                added_items_count += 1\n","            else:\n","                data_dict_nopssm[rbp_name] = {\n","                    'sequences': sequences,\n","                    'labels': labels\n","                }\n","                added_items_nopssm_count += 1  # Increment counter\n","                # Print the information for items added to data_dict_nopssm\n","                print(f\"Added to data_dict_nopssm: {rbp_name}, Total Items in data_dict_nopssm: {added_items_nopssm_count}\")\n","\n","            global_unique_characters.update(unique_characters)\n","            # print(f\"Finished processing {file_name}\")\n","\n","    print(f\"Items added from pssm_data_pca_dict to data_dict: {added_items_count}\")\n","    print(f\"Total items added to data_dict_nopssm: {added_items_nopssm_count}\")  # Final count for data_dict_nopssm\n","    return data_dict, data_dict_nopssm, global_unique_characters\n","\n","rbp_data_dict, rbp_data_dict_nopssm, global_unique_characters = process_all_files(RBP_path, pssm_data_pca_dict)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E7-v5K0j3oTx","executionInfo":{"status":"ok","timestamp":1719428404793,"user_tz":300,"elapsed":599995,"user":{"displayName":"Sasan Azizian","userId":"17610892618478316694"}},"outputId":"b48e8072-5d95-4d25-d83b-778c40ecd931"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Added to data_dict_nopssm: SMNDC1, Total Items in data_dict_nopssm: 1\n","Added to data_dict_nopssm: EIF3D, Total Items in data_dict_nopssm: 2\n","Added to data_dict_nopssm: NSUN2, Total Items in data_dict_nopssm: 3\n","Added to data_dict_nopssm: CPEB4, Total Items in data_dict_nopssm: 4\n","Added to data_dict_nopssm: BCLAF1, Total Items in data_dict_nopssm: 5\n","Added to data_dict_nopssm: HNRNPL, Total Items in data_dict_nopssm: 6\n","Added to data_dict_nopssm: EIF3G, Total Items in data_dict_nopssm: 7\n","Added to data_dict_nopssm: BCCIP, Total Items in data_dict_nopssm: 8\n","Added to data_dict_nopssm: AKAP8L, Total Items in data_dict_nopssm: 9\n","Added to data_dict_nopssm: METAP2, Total Items in data_dict_nopssm: 10\n","Added to data_dict_nopssm: DDX24, Total Items in data_dict_nopssm: 11\n","Added to data_dict_nopssm: AKAP1, Total Items in data_dict_nopssm: 12\n","Added to data_dict_nopssm: UTP18, Total Items in data_dict_nopssm: 13\n","Added to data_dict_nopssm: EIF4G2, Total Items in data_dict_nopssm: 14\n","Added to data_dict_nopssm: UCHL5, Total Items in data_dict_nopssm: 15\n","Added to data_dict_nopssm: LSM11, Total Items in data_dict_nopssm: 16\n","Added to data_dict_nopssm: PTBP1, Total Items in data_dict_nopssm: 17\n","Added to data_dict_nopssm: IGF2BP3, Total Items in data_dict_nopssm: 18\n","Added to data_dict_nopssm: DDX42, Total Items in data_dict_nopssm: 19\n","Added to data_dict_nopssm: SND1, Total Items in data_dict_nopssm: 20\n","Added to data_dict_nopssm: SRSF7, Total Items in data_dict_nopssm: 21\n","Added to data_dict_nopssm: GPKOW, Total Items in data_dict_nopssm: 22\n","Added to data_dict_nopssm: RBFOX2, Total Items in data_dict_nopssm: 23\n","Added to data_dict_nopssm: SAFB2, Total Items in data_dict_nopssm: 24\n","Added to data_dict_nopssm: STAU2, Total Items in data_dict_nopssm: 25\n","Added to data_dict_nopssm: TAF15, Total Items in data_dict_nopssm: 26\n","Added to data_dict_nopssm: ZC3H8, Total Items in data_dict_nopssm: 27\n","Added to data_dict_nopssm: SF3B4, Total Items in data_dict_nopssm: 28\n","Added to data_dict_nopssm: XRN2, Total Items in data_dict_nopssm: 29\n","Added to data_dict_nopssm: LIN28B, Total Items in data_dict_nopssm: 30\n","Added to data_dict_nopssm: XPO5, Total Items in data_dict_nopssm: 31\n","Added to data_dict_nopssm: HNRNPA1, Total Items in data_dict_nopssm: 32\n","Added to data_dict_nopssm: SUGP2, Total Items in data_dict_nopssm: 33\n","Added to data_dict_nopssm: ZNF622, Total Items in data_dict_nopssm: 34\n","Added to data_dict_nopssm: NONO, Total Items in data_dict_nopssm: 35\n","Added to data_dict_nopssm: SFPQ, Total Items in data_dict_nopssm: 36\n","Added to data_dict_nopssm: SDAD1, Total Items in data_dict_nopssm: 37\n","Added to data_dict_nopssm: LARP7, Total Items in data_dict_nopssm: 38\n","Added to data_dict_nopssm: CPSF6, Total Items in data_dict_nopssm: 39\n","Added to data_dict_nopssm: UTP3, Total Items in data_dict_nopssm: 40\n","Added to data_dict_nopssm: NIPBL, Total Items in data_dict_nopssm: 41\n","Added to data_dict_nopssm: SF3B1, Total Items in data_dict_nopssm: 42\n","Added to data_dict_nopssm: HNRNPC, Total Items in data_dict_nopssm: 43\n","Added to data_dict_nopssm: KHSRP, Total Items in data_dict_nopssm: 44\n","Added to data_dict_nopssm: EIF3H, Total Items in data_dict_nopssm: 45\n","Added to data_dict_nopssm: WRN, Total Items in data_dict_nopssm: 46\n","Added to data_dict_nopssm: SRSF1, Total Items in data_dict_nopssm: 47\n","Added to data_dict_nopssm: ZRANB2, Total Items in data_dict_nopssm: 48\n","Added to data_dict_nopssm: LARP4, Total Items in data_dict_nopssm: 49\n","Added to data_dict_nopssm: SSB, Total Items in data_dict_nopssm: 50\n","Added to data_dict_nopssm: ILF3, Total Items in data_dict_nopssm: 51\n","Added to data_dict_nopssm: CSTF2T, Total Items in data_dict_nopssm: 52\n","Added to data_dict_nopssm: PPIG, Total Items in data_dict_nopssm: 53\n","Added to data_dict_nopssm: U2AF2, Total Items in data_dict_nopssm: 54\n","Added to data_dict_nopssm: FMR1, Total Items in data_dict_nopssm: 55\n","Added to data_dict_nopssm: YWHAG, Total Items in data_dict_nopssm: 56\n","Added to data_dict_nopssm: U2AF1, Total Items in data_dict_nopssm: 57\n","Added to data_dict_nopssm: AGGF1, Total Items in data_dict_nopssm: 58\n","Added to data_dict_nopssm: SF3A3, Total Items in data_dict_nopssm: 59\n","Added to data_dict_nopssm: NCBP2, Total Items in data_dict_nopssm: 60\n","Added to data_dict_nopssm: DKC1, Total Items in data_dict_nopssm: 61\n","Added to data_dict_nopssm: FXR2, Total Items in data_dict_nopssm: 62\n","Added to data_dict_nopssm: TRA2A, Total Items in data_dict_nopssm: 63\n","Added to data_dict_nopssm: ZC3H11A, Total Items in data_dict_nopssm: 64\n","Added to data_dict_nopssm: SERBP1, Total Items in data_dict_nopssm: 65\n","Added to data_dict_nopssm: FXR1, Total Items in data_dict_nopssm: 66\n","Added to data_dict_nopssm: RPS11, Total Items in data_dict_nopssm: 67\n","Added to data_dict_nopssm: SUPV3L1, Total Items in data_dict_nopssm: 68\n","Added to data_dict_nopssm: GRSF1, Total Items in data_dict_nopssm: 69\n","Added to data_dict_nopssm: FKBP4, Total Items in data_dict_nopssm: 70\n","Added to data_dict_nopssm: ZNF800, Total Items in data_dict_nopssm: 71\n","Added to data_dict_nopssm: SUB1, Total Items in data_dict_nopssm: 72\n","Added to data_dict_nopssm: FUBP3, Total Items in data_dict_nopssm: 73\n","Added to data_dict_nopssm: NOLC1, Total Items in data_dict_nopssm: 74\n","Added to data_dict_nopssm: PPIL4, Total Items in data_dict_nopssm: 75\n","Added to data_dict_nopssm: WDR3, Total Items in data_dict_nopssm: 76\n","Added to data_dict_nopssm: UPF1, Total Items in data_dict_nopssm: 77\n","Added to data_dict_nopssm: DROSHA, Total Items in data_dict_nopssm: 78\n","Added to data_dict_nopssm: HNRNPUL1, Total Items in data_dict_nopssm: 79\n","Added to data_dict_nopssm: DDX52, Total Items in data_dict_nopssm: 80\n","Added to data_dict_nopssm: HNRNPU, Total Items in data_dict_nopssm: 81\n","Added to data_dict_nopssm: DDX51, Total Items in data_dict_nopssm: 82\n","Added to data_dict_nopssm: ABCF1, Total Items in data_dict_nopssm: 83\n","Added to data_dict_nopssm: DDX21, Total Items in data_dict_nopssm: 84\n","Added to data_dict_nopssm: QKI, Total Items in data_dict_nopssm: 85\n","Added to data_dict_nopssm: TROVE2, Total Items in data_dict_nopssm: 86\n","Added to data_dict_nopssm: TBRG4, Total Items in data_dict_nopssm: 87\n","Added to data_dict_nopssm: AUH, Total Items in data_dict_nopssm: 88\n","Added to data_dict_nopssm: XRCC6, Total Items in data_dict_nopssm: 89\n","Added to data_dict_nopssm: DGCR8, Total Items in data_dict_nopssm: 90\n","Added to data_dict_nopssm: GTF2F1, Total Items in data_dict_nopssm: 91\n","Added to data_dict_nopssm: CDC40, Total Items in data_dict_nopssm: 92\n","Added to data_dict_nopssm: PHF6, Total Items in data_dict_nopssm: 93\n","Added to data_dict_nopssm: MATR3, Total Items in data_dict_nopssm: 94\n","Added to data_dict_nopssm: MTPAP, Total Items in data_dict_nopssm: 95\n","Added to data_dict_nopssm: NIP7, Total Items in data_dict_nopssm: 96\n","Added to data_dict_nopssm: FUS, Total Items in data_dict_nopssm: 97\n","Added to data_dict_nopssm: NPM1, Total Items in data_dict_nopssm: 98\n","Added to data_dict_nopssm: DHX30, Total Items in data_dict_nopssm: 99\n","Added to data_dict_nopssm: WDR43, Total Items in data_dict_nopssm: 100\n","Added to data_dict_nopssm: POLR2G, Total Items in data_dict_nopssm: 101\n","Added to data_dict_nopssm: PABPC4, Total Items in data_dict_nopssm: 102\n","Added to data_dict_nopssm: SRSF9, Total Items in data_dict_nopssm: 103\n","Added to data_dict_nopssm: PRPF4, Total Items in data_dict_nopssm: 104\n","Added to data_dict_nopssm: HNRNPK, Total Items in data_dict_nopssm: 105\n","Added to data_dict_nopssm: DDX3X, Total Items in data_dict_nopssm: 106\n","Added to data_dict_nopssm: RBM15, Total Items in data_dict_nopssm: 107\n","Added to data_dict_nopssm: SBDS, Total Items in data_dict_nopssm: 108\n","Added to data_dict_nopssm: AATF, Total Items in data_dict_nopssm: 109\n","Added to data_dict_nopssm: DDX6, Total Items in data_dict_nopssm: 110\n","Added to data_dict_nopssm: DDX59, Total Items in data_dict_nopssm: 111\n","Added to data_dict_nopssm: PUM2, Total Items in data_dict_nopssm: 112\n","Added to data_dict_nopssm: RPS3, Total Items in data_dict_nopssm: 113\n","Added to data_dict_nopssm: PUM1, Total Items in data_dict_nopssm: 114\n","Added to data_dict_nopssm: RBM27, Total Items in data_dict_nopssm: 115\n","Added to data_dict_nopssm: HLTF, Total Items in data_dict_nopssm: 116\n","Added to data_dict_nopssm: EXOSC5, Total Items in data_dict_nopssm: 117\n","Added to data_dict_nopssm: PUS1, Total Items in data_dict_nopssm: 118\n","Added to data_dict_nopssm: RBM5, Total Items in data_dict_nopssm: 119\n","Added to data_dict_nopssm: CSTF2, Total Items in data_dict_nopssm: 120\n","Added to data_dict_nopssm: FTO, Total Items in data_dict_nopssm: 121\n","Added to data_dict_nopssm: GEMIN5, Total Items in data_dict_nopssm: 122\n","Added to data_dict_nopssm: SLTM, Total Items in data_dict_nopssm: 123\n","Added to data_dict_nopssm: GRWD1, Total Items in data_dict_nopssm: 124\n","Added to data_dict_nopssm: NKRF, Total Items in data_dict_nopssm: 125\n","Added to data_dict_nopssm: FAM120A, Total Items in data_dict_nopssm: 126\n","Added to data_dict_nopssm: GNL3, Total Items in data_dict_nopssm: 127\n","Added to data_dict_nopssm: EWSR1, Total Items in data_dict_nopssm: 128\n","Added to data_dict_nopssm: AARS, Total Items in data_dict_nopssm: 129\n","Added to data_dict_nopssm: PRPF8, Total Items in data_dict_nopssm: 130\n","Added to data_dict_nopssm: FASTKD2, Total Items in data_dict_nopssm: 131\n","Added to data_dict_nopssm: EFTUD2, Total Items in data_dict_nopssm: 132\n","Added to data_dict_nopssm: PCBP2, Total Items in data_dict_nopssm: 133\n","Added to data_dict_nopssm: RBM22, Total Items in data_dict_nopssm: 134\n","Added to data_dict_nopssm: SAFB, Total Items in data_dict_nopssm: 135\n","Added to data_dict_nopssm: TIAL1, Total Items in data_dict_nopssm: 136\n","Added to data_dict_nopssm: BUD13, Total Items in data_dict_nopssm: 137\n","Added to data_dict_nopssm: IGF2BP1, Total Items in data_dict_nopssm: 138\n","Added to data_dict_nopssm: NOL12, Total Items in data_dict_nopssm: 139\n","Added to data_dict_nopssm: IGF2BP2, Total Items in data_dict_nopssm: 140\n","Added to data_dict_nopssm: APOBEC3C, Total Items in data_dict_nopssm: 141\n","Added to data_dict_nopssm: PCBP1, Total Items in data_dict_nopssm: 142\n","Added to data_dict_nopssm: AQR, Total Items in data_dict_nopssm: 143\n","Added to data_dict_nopssm: G3BP1, Total Items in data_dict_nopssm: 144\n","Added to data_dict_nopssm: SLBP, Total Items in data_dict_nopssm: 145\n","Added to data_dict_nopssm: DDX55, Total Items in data_dict_nopssm: 146\n","Added to data_dict_nopssm: YBX3, Total Items in data_dict_nopssm: 147\n","Added to data_dict_nopssm: PABPN1, Total Items in data_dict_nopssm: 148\n","Added to data_dict_nopssm: KHDRBS1, Total Items in data_dict_nopssm: 149\n","Added to data_dict_nopssm: TIA1, Total Items in data_dict_nopssm: 150\n","Added to data_dict_nopssm: HNRNPM, Total Items in data_dict_nopssm: 151\n","Added to data_dict_nopssm: TARDBP, Total Items in data_dict_nopssm: 152\n","Added to data_dict_nopssm: TNRC6A, Total Items in data_dict_nopssm: 153\n","Added to data_dict_nopssm: RPS5, Total Items in data_dict_nopssm: 154\n","Added to data_dict_nopssm: Copy of AARS, Total Items in data_dict_nopssm: 155\n","Added to data_dict_nopssm: AGO2, Total Items in data_dict_nopssm: 156\n","Added to data_dict_nopssm: AGO1, Total Items in data_dict_nopssm: 157\n","Items added from pssm_data_pca_dict to data_dict: 0\n","Total items added to data_dict_nopssm: 157\n"]}]},{"cell_type":"code","source":["pssm_data_pca_dict = {\n","    \"SampleProtein1\": [0.1, 0.2, 0.3],\n","    \"SampleProtein2\": [0.4, 0.5, 0.6]\n","}\n","def find_similar_key(target, keys):\n","    # Simple heuristic: find a key with the maximum overlap in characters\n","    # This is a very basic form of similarity and might need adjustment\n","    similar_key = max(keys, key=lambda k: len(set(k) & set(target)))\n","    # Define a threshold for similarity, for example, at least half the characters match\n","    threshold = len(target) // 2\n","    if len(set(similar_key) & set(target)) >= threshold:\n","        return similar_key\n","    return None\n","\n","def process_all_files(directory_path, pssm_data_pca_dict):\n","    data_dict = {}\n","    data_dict_nopssm = {}\n","    global_unique_characters = set()\n","    added_items_count = 0\n","\n","    for file_name in os.listdir(directory_path):\n","        if file_name.endswith('.fa'):\n","            rbp_name = file_name.split('.')[0]\n","            file_path = os.path.join(directory_path, file_name)\n","            sequences, labels, unique_characters = read_and_process_data(file_path)\n","\n","            # Attempt to find an exact or similar key\n","            # key_to_use   = rbp_name if rbp_name in pssm_data_pca_dict else find_similar_key(rbp_name, pssm_data_pca_dict.keys())\n","            key_to_use = False\n","\n","            # Check if rbp_name and key_to_use are not the same, then print\n","            # if rbp_name != key_to_use:\n","                # print(f\"RBP Name: {rbp_name}, Key to Use: {key_to_use}\")\n","\n","            if key_to_use:\n","                data_dict[rbp_name] = {\n","                    'sequences': sequences,\n","                    'pssm_data': pssm_data_pca_dict[key_to_use],\n","                    'labels': labels\n","                }\n","                added_items_count += 1\n","            else:\n","                data_dict_nopssm[rbp_name] = {\n","                    'sequences': sequences,\n","                    'labels': labels\n","                }\n","\n","            global_unique_characters.update(unique_characters)\n","            print(f\"Finished processing {file_name}\")\n","\n","    print(f\"Items added from pssm_data_pca_dict to data_dict: {added_items_count}\")\n","    return data_dict, data_dict_nopssm, global_unique_characters\n","\n","rbp_data_dict, rbp_data_dict_nopssm, global_unique_characters = process_all_files(RBP_path, pssm_data_pca_dict)\n","\n","\n","import numpy as np\n","import os\n","\n","class PSSMProcessor:\n","    def __init__(self, path):\n","        self.path = path\n","        self.final_array = None\n","\n","    def load_and_adjust_pssms(self):\n","        def load_pssm(file):\n","            alphabet = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n","            with open(file, 'r') as f:\n","                lines = f.readlines()\n","            pssm = []\n","            for line in lines[3:-6]:\n","                pssm.append([int(x) for x in line.split()[2:22]])\n","            return np.array(pssm)\n","\n","        pssm_files = [os.path.join(self.path, f) for f in os.listdir(self.path) if f.endswith('.pssm')]\n","        pssm_data = []\n","\n","        for file in pssm_files:\n","            pssm_matrix = load_pssm(file)\n","            if pssm_matrix.size == 0:\n","                print(f\"File with zero-size PSSM matrix: {file}\")\n","            else:\n","                filename = os.path.basename(file)\n","                pssm_data.append((filename, pssm_matrix))\n","\n","        min_length, max_length, average_length = self.adjust_pssm_to_average_length(pssm_data)\n","        print(f\"Min length: {min_length}, Max length: {max_length}, Average length: {average_length}\")\n","        # Initialize pssm_dict here, right before filling it\n","        pssm_dict = {}\n","        for item in self.final_array:\n","            if \"-\" in item[0]:\n","                key_part = item[0].split('-')[0]\n","            else:\n","                key_part = item[0].split('_')[0]\n","            pssm_data = item[1]\n","            if len(pssm_data) == 0:\n","                print(f\"Empty PSSM data for file: {item[0]}, Data: {item[1]}\")\n","            else:\n","                pssm_dict[key_part] = pssm_data\n","\n","        return pssm_dict  # Ensure this return statement is included\n","\n","    def adjust_pssm_to_average_length(self, pssm_data):\n","        lengths = [pssm.shape[0] for _, pssm in pssm_data]\n","        min_length = min(lengths)\n","        max_length = max(lengths)\n","        average_length = sum(lengths) // len(lengths)\n","\n","        adjusted_pssms = []\n","        for filename, pssm in pssm_data:\n","            if pssm.shape[0] > average_length:\n","                adjusted_pssm = pssm[:average_length]\n","            elif pssm.shape[0] < average_length:\n","                repeat_times = average_length // pssm.shape[0] + 1\n","                extended_pssm = np.tile(pssm, (repeat_times, 1))[:average_length]\n","                adjusted_pssm = extended_pssm\n","            else:\n","                adjusted_pssm = pssm\n","            adjusted_pssms.append((filename, adjusted_pssm.flatten()))\n","\n","        self.final_array = np.array(adjusted_pssms, dtype=object)\n","        return min_length, max_length, average_length\n","\n","\n","processor = PSSMProcessor(pssm_path)\n","pssm_data_dict =processor.load_and_adjust_pssms()\n","# pssm_data_dict = processor.generate_random_rows(300)\n","print(f\"Number of entries in PSSM data dictionary: {len(pssm_data_dict)}\")\n","\n","# Print the first few entries to verify\n","for key in list(pssm_data_dict.keys())[:10]:  # Adjust the number to print as needed\n","    print(f\"Key: {key}, Length of PSSM row vector: {len(pssm_data_dict[key])}\")\n","    # print(f\"Key: {key}, Length of PSSM row vector: {len(pssm_data_dict[key])}, PSSM DATA: {pssm_data_dict[key]}\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"znc2LReV3pCp","outputId":"9e2be9b5-d509-45fc-eac2-9fbb808b87c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Finished processing SMNDC1.both.fa\n","Finished processing EIF3D.both.fa\n","Finished processing NSUN2.both.fa\n","Finished processing CPEB4.both.fa\n","Finished processing BCLAF1.both.fa\n","Finished processing HNRNPL.both.fa\n","Finished processing EIF3G.both.fa\n","Finished processing BCCIP.both.fa\n","Finished processing AKAP8L.both.fa\n","Finished processing METAP2.both.fa\n","Finished processing DDX24.both.fa\n","Finished processing AKAP1.both.fa\n","Finished processing UTP18.both.fa\n","Finished processing EIF4G2.both.fa\n","Finished processing UCHL5.both.fa\n","Finished processing LSM11.both.fa\n","Finished processing PTBP1.both.fa\n","Finished processing IGF2BP3.both.fa\n","Finished processing DDX42.both.fa\n","Finished processing SND1.both.fa\n","Finished processing SRSF7.both.fa\n","Finished processing GPKOW.both.fa\n","Finished processing RBFOX2.both.fa\n","Finished processing SAFB2.both.fa\n","Finished processing STAU2.both.fa\n","Finished processing TAF15.both.fa\n","Finished processing ZC3H8.both.fa\n","Finished processing SF3B4.both.fa\n","Finished processing XRN2.both.fa\n","Finished processing LIN28B.both.fa\n","Finished processing XPO5.both.fa\n","Finished processing HNRNPA1.both.fa\n","Finished processing SUGP2.both.fa\n","Finished processing ZNF622.both.fa\n","Finished processing NONO.both.fa\n","Finished processing SFPQ.both.fa\n","Finished processing SDAD1.both.fa\n","Finished processing LARP7.both.fa\n","Finished processing CPSF6.both.fa\n","Finished processing UTP3.both.fa\n","Finished processing NIPBL.both.fa\n","Finished processing SF3B1.both.fa\n","Finished processing HNRNPC.both.fa\n","Finished processing KHSRP.both.fa\n","Finished processing EIF3H.both.fa\n","Finished processing WRN.both.fa\n","Finished processing SRSF1.both.fa\n","Finished processing ZRANB2.both.fa\n","Finished processing LARP4.both.fa\n","Finished processing SSB.both.fa\n","Finished processing ILF3.both.fa\n","Finished processing CSTF2T.both.fa\n","Finished processing PPIG.both.fa\n","Finished processing U2AF2.both.fa\n","Finished processing FMR1.both.fa\n","Finished processing YWHAG.both.fa\n","Finished processing U2AF1.both.fa\n","Finished processing AGGF1.both.fa\n","Finished processing SF3A3.both.fa\n","Finished processing NCBP2.both.fa\n","Finished processing DKC1.both.fa\n","Finished processing FXR2.both.fa\n","Finished processing TRA2A.both.fa\n","Finished processing ZC3H11A.both.fa\n","Finished processing SERBP1.both.fa\n","Finished processing FXR1.both.fa\n","Finished processing RPS11.both.fa\n","Finished processing SUPV3L1.both.fa\n","Finished processing GRSF1.both.fa\n","Finished processing FKBP4.both.fa\n","Finished processing ZNF800.both.fa\n","Finished processing SUB1.both.fa\n","Finished processing FUBP3.both.fa\n","Finished processing NOLC1.both.fa\n","Finished processing PPIL4.both.fa\n","Finished processing WDR3.both.fa\n","Finished processing UPF1.both.fa\n","Finished processing DROSHA.both.fa\n","Finished processing HNRNPUL1.both.fa\n","Finished processing DDX52.both.fa\n","Finished processing HNRNPU.both.fa\n","Finished processing DDX51.both.fa\n","Finished processing ABCF1.both.fa\n","Finished processing DDX21.both.fa\n","Finished processing QKI.both.fa\n","Finished processing TROVE2.both.fa\n","Finished processing TBRG4.both.fa\n","Finished processing AUH.both.fa\n","Finished processing XRCC6.both.fa\n","Finished processing DGCR8.both.fa\n","Finished processing GTF2F1.both.fa\n","Finished processing CDC40.both.fa\n","Finished processing PHF6.both.fa\n","Finished processing MATR3.both.fa\n","Finished processing MTPAP.both.fa\n","Finished processing NIP7.both.fa\n","Finished processing FUS.both.fa\n","Finished processing NPM1.both.fa\n"]}]},{"cell_type":"code","source":["processor = PSSMProcessor(pssm_path)\n","pssm_data_dict =processor.load_and_adjust_pssms()\n","# pssm_data_dict = processor.generate_random_rows(300)\n","print(f\"Number of entries in PSSM data dictionary: {len(pssm_data_dict)}\")\n","\n","# Print the first few entries to verify\n","for key in list(pssm_data_dict.keys())[:10]:  # Adjust the number to print as needed\n","    print(f\"Key: {key}, Length of PSSM row vector: {len(pssm_data_dict[key])}\")\n","    # print(f\"Key: {key}, Length of PSSM row vector: {len(pssm_data_dict[key])}, PSSM DATA: {pssm_data_dict[key]}\")\n","\n","# prompt: applay PCA to df and give anaysis of elbow curve for optimaration\n","\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","\n","# Apply PCA to the data\n","pca = PCA()\n","pca.fit(df)\n","\n","# Plot the elbow curve\n","plt.plot(range(1, len(df.columns) + 1), pca.explained_variance_ratio_, marker='o')\n","plt.xlabel('Principal Component')\n","plt.ylabel('Explained Variance Ratio')\n","plt.title('Elbow Curve for PSSM')\n","plt.show()\n","\n","# Analyze the elbow curve to determine the optimal number of principal components to retain\n","# Look for the point where the explained variance ratio starts to level off\n","# In this case, it appears that retaining around 100 principal components would capture most of the variance in the data\n","\n","# Perform PCA with the chosen number of components\n","\n","pca = PCA(n_components=25)\n","df_pca = pca.fit_transform(df)\n","\n","# Print the transformed data\n","print(df_pca[0])\n","type(df_pca)\n","\n","import pandas as pd\n","from sklearn.decomposition import PCA\n","\n","# Assuming pssm_data_dict is already filled with your data\n","# Convert the dictionary values (which are the PSSM arrays) into a list of lists\n","data = [value for key, value in pssm_data_dict.items()]\n","\n","# Create the DataFrame from this data\n","df = pd.DataFrame(data)\n","# Assuming 'num_files' is defined; if not, replace df.columns[:num_files] with appropriate slicing or remove if unnecessary\n","# df = df[df.columns[:num_files]] # Uncomment if num_files is defined and you need to slice the DataFrame\n","\n","# Rename the columns as specified\n","df.columns = ['x' + str(col) for col in df.columns]\n","# print(df.head())\n","\n","# Apply PCA to the data\n","pca = PCA(n_components=25)  # Directly initializing PCA with n_components=25\n","df_pca = pca.fit_transform(df)\n","\n","\n","# Print the explained variance ratio\n","print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n","# Creating a new dictionary with the same keys and the PCA-transformed data as values\n","keys = list(pssm_data_dict.keys())\n","pssm_data_pca_dict = {keys[i]: df_pca[i] for i in range(len(keys))}\n","\n","# Now, pssm_data_pca_dict contains the original keys and the PCA-transformed data\n","print(f\"Number of entries in PSSM data dictionary: {len(pssm_data_pca_dict)}\")\n","\n","# Print the first few entries to verify\n","for key in list(pssm_data_pca_dict.keys())[:1]:  # Adjust the number to print as needed\n","    print(f\"Key: {key}, Length of PSSM row vector: {len(pssm_data_pca_dict[key])}, PSSM DATA: {pssm_data_pca_dict[key]}\")"],"metadata":{"id":"M4S-tTu540zy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to normalize data and train an autoencoder\n","def train_autoencoder(data, encoding_dim):\n","    # Scale the data to range [-1, 1]\n","    scaler = MinMaxScaler(feature_range=(-1, 1))\n","    data_normalized = scaler.fit_transform(data)\n","\n","    # Define the input layer\n","    input_img = tf.keras.Input(shape=(data_normalized.shape[1],))\n","    # Encoder layers\n","    encoded = layers.Dense(encoding_dim * 2, activation='relu')(input_img)\n","    encoded = layers.Dense(encoding_dim, activation='relu')(encoded)\n","    # Decoder layers\n","    decoded = layers.Dense(encoding_dim * 2, activation='relu')(encoded)\n","    decoded = layers.Dense(data_normalized.shape[1], activation='tanh')(decoded)  # Using tanh activation\n","\n","    # Autoencoder model\n","    autoencoder = models.Model(input_img, decoded)\n","    # Encoder model\n","    encoder = models.Model(input_img, encoded)\n","\n","    # Compile the autoencoder\n","    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n","\n","    # Train the autoencoder\n","    autoencoder.fit(data_normalized, data_normalized,\n","                    epochs=100,\n","                    batch_size=256,\n","                    shuffle=True,\n","                    validation_split=0.2)\n","\n","    # Use the encoder to reduce the dimensionality of the data\n","    reduced_data = encoder.predict(data_normalized)\n","\n","    return reduced_data, encoder, autoencoder, scaler\n","\n","\n","\n","# Assuming pssm_data_dict is your dictionary of PSSM data\n","keys = list(pssm_data_dict.keys())\n","pssm_values = np.array(list(pssm_data_dict.values()))\n","\n","# Assuming all PSSM vectors are of the same length, otherwise you'll need to adjust them\n","# Determine the desired encoding dimension\n","encoding_dim = int(pssm_values.shape[1] * 0.2)  # Example: 20% of the input size\n","\n","# Train the autoencoder and get the reduced data\n","reduced_data, encoder, autoencoder, scaler = train_autoencoder(pssm_values, encoding_dim)\n","\n","# Map the reduced data back to the protein names\n","reduced_data_dict = {key: reduced_data[i] for i, key in enumerate(keys)}\n","\n","\n","# Example: Print the first few entries to verify\n","# for key in list(reduced_data_dict.keys())[:5]:\n","#     print(f\"Protein: {key}, Reduced PSSM Data: {reduced_data_dict[key]}\")\n"],"metadata":{"id":"RE46xfVC5g4y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","\n","def evaluate_and_visualize(model, X_test_seq, X_test_pssm, y_test, protein_name):\n","    # Generate predictions\n","    test_pred = (model.predict([X_test_seq, X_test_pssm]) > 0.5).astype(int)\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(y_test, test_pred)\n","    precision = precision_score(y_test, test_pred, zero_division=0)\n","    recall = recall_score(y_test, test_pred, zero_division=0)\n","    f1 = f1_score(y_test, test_pred, zero_division=0)\n","\n","    # Print metrics\n","    print(f\"\\nResults for {protein_name}:\")\n","    print(f\"Accuracy: {accuracy}\")\n","    print(f\"Precision: {precision}\")\n","    print(f\"Recall: {recall}\")\n","    print(f\"F1 Score: {f1}\")\n","\n","    # Plot confusion matrix\n","    conf_matrix = confusion_matrix(y_test, test_pred)\n","    plt.figure(figsize=(6, 5))\n","    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n","    plt.title(f'Confusion Matrix for {protein_name}')\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.tight_layout()\n","    plt.show()\n","\n","def plot_training_history(history, title=''):\n","    # Plot accuracy\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(history.history['accuracy'], label='Training Accuracy')\n","    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","    plt.title(f'Model Accuracy for {title}')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(loc='lower right')\n","\n","    # Plot loss\n","    plt.subplot(1, 2, 2)\n","    plt.plot(history.history['loss'], label='Training Loss')\n","    plt.plot(history.history['val_loss'], label='Validation Loss')\n","    plt.title(f'Model Loss for {title}')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(loc='upper right')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","class Metrics(Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        train_pred = (self.model.predict(X_train) > 0.5).astype(int)\n","        test_pred = (self.model.predict(X_test) > 0.5).astype(int)\n","        metrics = {\n","            \"Training\": (y_train, train_pred),\n","            \"Testing\": (y_test, test_pred),\n","        }\n","        for key, (y, pred) in metrics.items():\n","            accuracy = accuracy_score(y, pred)\n","            precision = precision_score(y, pred, zero_division=0)\n","            recall = recall_score(y, pred, zero_division=0)\n","            f1 = f1_score(y, pred, zero_division=0)\n","            print(f\"\\n{key} - Epoch: {epoch+1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")"],"metadata":{"id":"ozi9jqES536n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Attention(Layer):\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n","        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n","        super().build(input_shape)\n","\n","    def call(self, x):\n","        e = K.tanh(K.dot(x, self.W) + self.b)\n","        a = K.softmax(e, axis=1)\n","        output = x * a\n","        return K.sum(output, axis=1)\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape[0], input_shape[-1]\n","\n","\n","def create_model_and_embedding_function(max_length, unique_characters, embedding_dim=128, lstm_units=512, dropout_rate=0.5, learning_rate=0.0001):\n","    sequence_input = Input(shape=(max_length,), name='sequences')\n","    x = Embedding(input_dim=len(unique_characters) + 1, output_dim=embedding_dim, input_length=max_length)(sequence_input)\n","    x = Bidirectional(LSTM(lstm_units, return_sequences=True))(x)\n","    x = Dropout(dropout_rate)(x)\n","    x = Bidirectional(LSTM(lstm_units, return_sequences=True))(x)\n","    x = Dropout(dropout_rate)(x)\n","    x = Bidirectional(LSTM(lstm_units, return_sequences=True))(x)\n","    x = Dropout(dropout_rate)(x)\n","    x = Attention()(x)\n","    x = Dropout(dropout_rate)(x)\n","    outputs = Dense(64, activation='relu')(x)\n","    outputs = Dropout(dropout_rate)(outputs)\n","    final_output = Dense(1, activation='sigmoid')(outputs)\n","\n","    model = Model(inputs=[sequence_input], outputs=final_output)\n","    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])\n","\n","    # Define a function to get embeddings from the LSTM layer\n","    embedding_model = Model(inputs=[sequence_input], outputs=x_lstm)\n","\n","    return model, embedding_model\n","\n","#Save Model\n","\n","def get_callbacks(model_filepath):\n","  callbacks = [\n","    # EarlyStopping(monitor=\"val_loss\", patience=30),\n","    ModelCheckpoint(\n","        filepath=model_filepath,\n","        monitor=\"val_loss\",\n","        save_best_only=True,\n","        save_weights_only=False,\n","        mode='auto',\n","        period=1)\n","    ]\n","\n","  return callbacks\n","\n","model_filepath = \"/content/drive/My Drive/Data_Final_Code/mirna_model.h.Jun12\"\n","callbacks = get_callbacks(model_filepath)\n","\n","#Create train and test\n","\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","# Placeholder for combined data\n","all_sequences = []\n","all_labels = []\n","\n","# Combine data from all keys\n","for key in rbp_data_dict_nopssm:\n","    all_sequences.extend(rbp_data_dict_nopssm[key]['sequences'])\n","    all_labels.extend(rbp_data_dict_nopssm[key]['labels'])\n","\n","# Convert lists to NumPy arrays\n","all_sequences = np.array(all_sequences)\n","all_labels = np.array(all_labels)\n","\n","# Split into training and test sets\n","X_train_seq, X_test_seq, y_train, y_test = train_test_split(\n","    all_sequences, all_labels, test_size=0.1, random_state=42\n",")\n","# Print shapes to verify the split\n","print(f'Training sequences shape: {X_train_seq.shape}')\n","print(f'Test sequences shape: {X_test_seq.shape}')\n","print(f'Training labels shape: {y_train.shape}')\n","print(f'Test labels shape: {y_test.shape}')\n","\n","\n","\n","# Calculate max_length\n","max_length = max(len(seq) for key in rbp_data_dict_nopssm for seq in rbp_data_dict_nopssm[key]['sequences'])\n","\n","print(f\"max_length: {max_length}, num_unique_characters: {global_unique_characters}\")\n","\n","# Assuming create_model_with_pssm function is defined as in your snippet\n","model, embedding_model = create_model_and_embedding_function(max_length, global_unique_characters)\n","model.summary()\n","\n","\n","# Save model summary to a file\n","import sys\n","from contextlib import redirect_stdout\n","\n","summary_path = '/content/drive/My Drive/model_summary.txt'\n","\n","with open(summary_path, 'w') as f:\n","    with redirect_stdout(f):\n","        model.summary()\n","\n","\n","\n","\n","history = model.fit([X_train_seq], y_train, validation_data=([X_test_seq], y_test), batch_size=64, epochs=50,callbacks=callbacks)\n","# history = model.fit([X_train_seq], y_train, validation_data=([X_test_seq], y_test), batch_size=32, epochs=35,callbacks=callbacks)\n","\n","\n","import pandas as pd\n","\n","history_df = pd.DataFrame(history.history)\n","history_path = '/content/drive/My Drive/training_history.csv'\n","history_df.to_csv(history_path, index=False)"],"metadata":{"id":"WSNeTWA257Ck"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n","import tensorflow as tf\n","import pandas as pd\n","\n","\n","\n","# Calculate max_length\n","max_length = max(len(seq) for key in rbp_data_dict_nopssm for seq in rbp_data_dict_nopssm[key]['sequences'])\n","\n","print(f\"max_length: {max_length}, num_unique_characters: {global_unique_characters}\")\n","\n","# Assuming create_model_with_pssm function is defined as in your snippet\n","model, embedding_model = create_model_and_embedding_function(max_length, global_unique_characters)\n","model.summary()\n","\n","\n","# Save model summary to a file\n","import sys\n","from contextlib import redirect_stdout\n","\n","summary_path = '/content/drive/My Drive/model_summary.txt'\n","\n","with open(summary_path, 'w') as f:\n","    with redirect_stdout(f):\n","        model.summary()\n","\n","\n","\n","# Define the checkpoint and logger paths\n","checkpoint_path = '/content/drive/My Drive/model_checkpoints/cp-{epoch:04d}.ckpt'\n","history_path = '/content/drive/My Drive/training_history.csv'\n","\n","# Define the checkpoint callback\n","checkpoint_callback = ModelCheckpoint(\n","    filepath=checkpoint_path,\n","    save_weights_only=True,\n","    save_freq='epoch',\n","    verbose=1\n",")\n","\n","# Define a CSV logger to save training history\n","csv_logger = CSVLogger(history_path, append=True)\n","\n","# Create the model\n","model, embedding_model = create_model_and_embedding_function(max_length, global_unique_characters)\n","\n","# Load the latest checkpoint if available\n","latest_checkpoint = tf.train.latest_checkpoint('/content/drive/My Drive/model_checkpoints')\n","if latest_checkpoint:\n","    model.load_weights(latest_checkpoint)\n","    print(f\"Resuming training from checkpoint: {latest_checkpoint}\")\n","\n","# Get the initial epoch from the latest checkpoint\n","initial_epoch = int(latest_checkpoint.split('-')[-1].split('.')[0]) if latest_checkpoint else 0\n","\n","# Train the model\n","history = model.fit(\n","    [X_train_seq],\n","    y_train,\n","    validation_data=([X_test_seq], y_test),\n","    batch_size=64,\n","    epochs=50,\n","    initial_epoch=initial_epoch,\n","    callbacks=[checkpoint_callback, csv_logger]\n",")\n","\n","# Manually save the model if needed\n","model.save('/content/drive/My Drive/model.h500')\n"],"metadata":{"id":"Ur3c0Hxa6eCQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_and_visualize(model, X_seq, y, dataset_name):\n","    # Generate predictions\n","    predictions = (model.predict([X_seq]) > 0.5).astype(int)\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(y, predictions)\n","    precision = precision_score(y, predictions, zero_division=0)\n","    recall = recall_score(y, predictions, zero_division=0)\n","    f1 = f1_score(y, predictions, zero_division=0)\n","\n","    # Print metrics\n","    print(f\"\\nResults for {dataset_name}:\")\n","    print(f\"Accuracy: {accuracy}\")\n","    print(f\"Precision: {precision}\")\n","    print(f\"Recall: {recall}\")\n","    print(f\"F1 Score: {f1}\")\n","\n","    # Plot confusion matrix\n","    conf_matrix = confusion_matrix(y, predictions)\n","    plt.figure(figsize=(6, 5))\n","    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n","    plt.title(f'Confusion Matrix for {dataset_name}')\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.show()\n","\n","# Evaluate and visualize for training dataset\n","evaluate_and_visualize(model, X_train_seq, y_train, \"Training Set\")\n","\n","# Evaluate and visualize for test dataset\n","evaluate_and_visualize(model, X_test_seq, y_test, \"Test Set\")\n","\n","plot_training_history(history, title='Model Performance')\n"],"metadata":{"id":"jJG91RoJ69eu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","def evaluate_and_visualize_per_protein(model, X_test_seq, X_test_pssm, y_test, protein_name):\n","    predictions = (model.predict([X_test_seq, X_test_pssm]) > 0.5).astype(int)\n","\n","    accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, zero_division=0)\n","    recall = recall_score(y_test, predictions, zero_division=0)\n","    f1 = f1_score(y_test, predictions, zero_division=0)\n","\n","    print(f\"Results for {protein_name}:\")\n","    print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n","\n","    conf_matrix = confusion_matrix(y_test, predictions)\n","    sns.heatmap(conf_matrix, annot=True, fmt='d')\n","    plt.title(f'Confusion Matrix for {protein_name}')\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.show()\n","from sklearn.model_selection import train_test_split\n","\n","# Assuming rbp_data_dict_nopssm is your dataset organized per protein\n","for protein_name, protein_data in rbp_data_dict_nopssm.items():\n","    # Prepare the data for the current protein\n","    sequences = protein_data['sequences']\n","    labels = protein_data['labels']\n","\n","    sequences = np.array(sequences)\n","    labels = np.array(labels)\n","\n","\n","    # Splitting the data into training and test sets for this specific protein\n","    X_train_seq, X_test_seq, y_train, y_test = train_test_split(\n","        sequences, labels, test_size=0.2, random_state=42\n","    )\n","    print(f\"X_train_seq Sahpe: {X_train_seq.shape}:   y_train: {y_train.shape}\")\n","    print(f\"X_test_seq Sahpe:  {X_test_seq.shape}:     y_test: {y_test.shape}\" )\n","\n","\n","    # Assuming you have a function to create the model\n","    model = create_model_with_pssm(max_length, global_unique_characters)\n","    # Train the model for the current protein\n","    # Fit the model on the training data\n","    history = model.fit([X_train_seq], y_train,\n","                        validation_data=([X_test_seq], y_test),\n","                        batch_size=32, epochs=1)\n","\n","    # Visualize the training process\n","    plot_training_history(history, title=f'Model Performance for {protein_name}')\n","\n","    # Evaluate and visualize for the current protein\n","    evaluate_and_visualize_per_protein(model, X_test_seq, y_test, protein_name)\n"],"metadata":{"id":"7TsN14SH7A9n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","def sample_positive_sequences(rbp_data_dict_nopssm, sample_size=1000):\n","\n","    rbp_data_dict_sampled = {}\n","\n","    for protein_name, data in rbp_data_dict_nopssm.items():\n","        sequences = data['sequences']\n","        labels = data['labels']\n","\n","        # Combine sequences and labels into a single list of tuples\n","        combined_data = list(zip(sequences, labels))\n","\n","        # Filter to only include sequences with label 1\n","        positive_samples = [item for item in combined_data if item[1] == 1]\n","\n","        # Randomly select up to `sample_size` samples\n","        num_samples_to_select = min(sample_size, len(positive_samples))\n","        sampled_positive_sequences = random.sample(positive_samples, num_samples_to_select)\n","\n","        # Unzip the sequences and labels back into separate lists\n","        sampled_sequences, sampled_labels = zip(*sampled_positive_sequences) if sampled_positive_sequences else ([], [])\n","\n","        # Add the sampled data to the new dictionary\n","        rbp_data_dict_sampled[protein_name] = {\n","            'sequences': list(sampled_sequences),\n","            'labels': list(sampled_labels)\n","        }\n","\n","    return rbp_data_dict_sampled\n","\n","# Example usage:\n","# rbp_data_dict_nopssm is your existing dictionary\n","sampled_rna_data_dict = sample_positive_sequences(rbp_data_dict_nopssm, 1000)\n","import random\n","\n","def sample_positive_sequences(rbp_data_dict_nopssm, sample_size=1000):\n","\n","    rbp_data_dict_sampled = {}\n","\n","    for protein_name, data in rbp_data_dict_nopssm.items():\n","        sequences = data['sequences']\n","        labels = data['labels']\n","\n","        # Combine sequences and labels into a single list of tuples\n","        combined_data = list(zip(sequences, labels))\n","\n","        # Filter to only include sequences with label 1\n","        positive_samples = [item for item in combined_data if item[1] == 1]\n","\n","        # Randomly select up to `sample_size` samples\n","        num_samples_to_select = min(sample_size, len(positive_samples))\n","        sampled_positive_sequences = random.sample(positive_samples, num_samples_to_select)\n","\n","        # Unzip the sequences and labels back into separate lists\n","        sampled_sequences, sampled_labels = zip(*sampled_positive_sequences) if sampled_positive_sequences else ([], [])\n","\n","        # Add the sampled data to the new dictionary\n","        rbp_data_dict_sampled[protein_name] = {\n","            'sequences': list(sampled_sequences),\n","            'labels': list(sampled_labels)\n","        }\n","\n","    return rbp_data_dict_sampled\n","\n","# Example usage:\n","# rbp_data_dict_nopssm is your existing dictionary\n","sampled_rna_data_dict = sample_positive_sequences(rbp_data_dict_nopssm, 1000)\n","sampled_rna_data_dict['AATF']['sequences'][0].shape\n","\n","#Load model\n","from tensorflow.keras.models import Model\n","## load model\n","model_filepath = '/content/drive/My Drive/model.h500'\n","rna_model = load_model(model_filepath, custom_objects={'Attention': Attention})\n","\n","\n","\n","\n","for i, layer in enumerate(rna_model.layers):\n","    print(i, layer.name, layer.output_shape)\n","\n","\n","embedding_layer_name = 'attention'  # or whatever the correct name is, based on the printed names\n","embedding_layer_output = rna_model.get_layer(embedding_layer_name).output\n","\n","# # Create a new model for embeddings\n","embedding_model = Model(inputs=rna_model.input, outputs=embedding_layer_output)\n","\n","from tensorflow.keras.models import Model\n","## load model\n","model_filepath = '/content/drive/My Drive/model.h500'\n","checkpoint_path = '/content/drive/My Drive/model_checkpoints'\n","rna_model = load_model(model_filepath, custom_objects={'Attention': Attention})\n","\n","# If you need to load the weights from the latest checkpoint\n","latest_checkpoint = tf.train.latest_checkpoint(checkpoint_path)\n","if latest_checkpoint:\n","    rna_model.load_weights(latest_checkpoint)\n","    print(f\"Loaded weights from checkpoint: {latest_checkpoint}\")\n","\n","# Verify the model structure\n","rna_model.summary()\n","\n","\n","for i, layer in enumerate(rna_model.layers):\n","    print(i, layer.name, layer.output_shape)\n","\n","\n","# or whatever the correct name is, based on the printed names\n","embedding_layer_output = rna_model.get_layer('attention_1').output\n","\n","# # Create a new model for embeddings\n","embedding_model = Model(inputs=rna_model.input, outputs=embedding_layer_output)\n","\n"],"metadata":{"id":"rlJ7JUWR7F5C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_embeddings_and_probabilities(sampled_data_dict, model, embedding_model, max_length=101):\n","    embedding_rna_samples_dict = {}\n","\n","    for key, data in sampled_data_dict.items():\n","        sequences = np.array(data['sequences'])\n","\n","        # Check if reshaping is needed based on your model's input requirements\n","        # processed_sequences = sequences  # Assume sequences are already in the correct shape\n","        # Pad sequences to match the input length expected by the model\n","        processed_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post', dtype='int32')\n","\n","        # Get embeddings\n","        embeddings = embedding_model.predict(processed_sequences)\n","\n","        # Get probabilities\n","        probabilities = model.predict(processed_sequences).flatten()\n","\n","        embedding_rna_samples_dict[key] = {\n","            'embeddings': embeddings,\n","            'probabilities': probabilities\n","        }\n","\n","    return embedding_rna_samples_dict\n","\n","# Now use the function with your loaded model and embedding model\n","embedding_rna_samples_dict = get_embeddings_and_probabilities(\n","    sampled_rna_data_dict, rna_model, embedding_model\n",")\n","for key, value in embedding_rna_samples_dict.items():\n","    print(f\"Protein: {key}\")\n","    embeddings = value['embeddings']\n","    probabilities = value['probabilities']\n","    print(f\"Embeddings Shape: {embeddings.shape}\")\n","    print(f\"First Embedding Sample: {embeddings[0]}\")\n","    print(f\"Probabilities Shape: {probabilities.shape}\")\n","    print(f\"First Probability Sample: {probabilities[0]}\")\n","    print(\"\\n\")\n","\n"],"metadata":{"id":"CU7cBf2K7UzQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","def sum_embeddings(embedding_dict):\n","    # Initialize a new dictionary to store the results\n","    summed_embeddings_dict = {}\n","\n","    # Iterate over each key in the input dictionary\n","    for key, value in embedding_dict.items():\n","        print(f\"Processing Protein: {key}\")\n","        embeddings = value['embeddings']\n","\n","        # Sum embeddings along axis 1\n","        summed_embeddings = np.sum(embeddings, axis=1)\n","\n","        # Save the summed embeddings in the new dictionary\n","        summed_embeddings_dict[key] = {\n","            'summed_embeddings': summed_embeddings\n","        }\n","\n","        # Print the shape to verify\n","        print(f\"Summed Embeddings Shape: {summed_embeddings.shape}\")\n","\n","    return summed_embeddings_dict\n","\n","# Call the function with the example input\n","summed_embeddings_dict = sum_embeddings(embedding_rna_samples_dict)\n","\n","# Check the results\n","for key, value in summed_embeddings_dict.items():\n","    print(f\"Protein: {key}\")\n","    summed_embeddings = value['summed_embeddings']\n","    print(f\"Summed Embeddings Shape: {summed_embeddings.shape}\")\n","summed_embeddings_dict['AATF']['summed_embeddings'][0].shape"],"metadata":{"id":"8YzmP7327bWt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check if 'AKAP8L' exists in final_data\n","if 'AKAP8L' in embedding_rna_samples_dict:\n","    bclaf1_data = embedding_rna_samples_dict['AKAP8L']  # Retrieve data for 'BCLAF1'\n","    embeddings = bclaf1_data['embeddings']\n","    probabilities = bclaf1_data['probabilities']\n","\n","    # Ensure there's data to print\n","    num_rows_to_print = min(5, len(embeddings))  # Print for 2 rows or fewer if not enough data\n","\n","    for i in range(num_rows_to_print):\n","        print(f\"Row {i+1} for 'BCLAF1':\")\n","        print(\"embeddings:\", embeddings[i])\n","        print(\"probabilities:\", probabilities[i])\n","        print()  # Print a newline for better readability between rows\n","else:\n","    print(\"Key 'BCLAF1' not found in final_data.\")"],"metadata":{"id":"f33Cfx2n7fQb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Read the file into a pandas DataFrame\n","df = pd.read_csv(miRNA_path, sep='\\t')\n","\n","# Now df contains your data and you can work with it as needed\n","# For example, to display the first few rows of the DataFrame, you can use:\n","print(df.head(20))\n","\n","# If you need to use the DataFrame in other parts of your code, it's now stored in the variable 'df'\n","miRNA_sequences, miRNA_labels, miRNA_unique_characters = read_and_process_data(miRNA_path)\n","\n","\n","# Creating the dictionary as specified by the user\n","protein_name = \"AGO\"\n","miRNA_data_dict_sampled = {\n","    protein_name: {\n","        'sequences': miRNA_sequences.tolist(),  # Converting the numpy array to a list\n","        'labels': miRNA_labels.tolist()  # Converting the numpy array to a list\n","    }\n","}\n","\n","# Confirming the keys and types to ensure it's correctly structured\n","structure_confirmation = {protein_name: {'sequences_type': type(miRNA_data_dict_sampled[protein_name]['sequences']),\n","                                         'labels_type': type(miRNA_data_dict_sampled[protein_name]['labels'])}}\n","\n","structure_confirmation\n","\n","for key in list(miRNA_data_dict_sampled.keys())[:5]:  # Adjust the number to print as needed\n","    print(f\"Key: {key}, Length of data: {len(miRNA_data_dict_sampled[key]['sequences'])}\")\n","\n","# Now use the function with your loaded model and embedding model\n","embedding_miRNA_dict = get_embeddings_and_probabilities(\n","    miRNA_data_dict_sampled, rna_model, embedding_model\n",")\n","\n","\n","for key, value in embedding_miRNA_dict.items():\n","    print(f\"Protein: {key}\")\n","    embeddings = value['embeddings']\n","    probabilities = value['probabilities']\n","    print(f\"Embeddings Shape: {embeddings[1]}\")\n","    print(f\"First Embedding Sample: {embeddings[3]}\")\n","    print(f\"Probabilities Shape: {probabilities.shape}\")\n","    print(f\"First Probability Sample: {probabilities[1]}\")\n","    print(\"\\n\")\n"],"metadata":{"id":"Lkxf1XHL7mty"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#READ miRNA2 with using the differend MODEL\n","import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# miRNA_path = '/content/drive/My Drive/Data_Final_Code/miRNA-DATA/complementary_miRNA_sequences_ago.txt'\n","# miRNA_path ='/content/drive/My Drive/Data_Final_Code/miRNA-DATA/miR-223.txt'\n","miRNA_path ='/content/drive/My Drive/Data_Final_Code/miRNA-DATA/let2-7d.txt'\n","# miRNA_path ='/content/drive/My Drive/Data_Final_Code/miRNA-DATA/complementary-let-7d.txt'\n","\n","def process_miRNA_sequences(file_path, RBP_name, max_length=None):\n","    # Load miRNA sequences\n","    miRNA_df = pd.read_csv(file_path, sep='\\t')\n","\n","    # Tokenize the sequences\n","    tokenizer = Tokenizer(char_level=True)  # Use char_level for nucleotide sequences\n","    tokenizer.fit_on_texts(miRNA_df['sequence'])\n","\n","    # Convert sequences to numerical format\n","    sequences = tokenizer.texts_to_sequences(miRNA_df['sequence'])\n","\n","    # Determine max length if not specified\n","    if max_length is None:\n","        max_length = max(len(seq) for seq in sequences)\n","\n","    # Pad sequences to have the same length\n","    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n","\n","    # Calculate lengths of each sequence\n","    lengths = [len(seq) for seq in sequences]\n","\n","    # # Labels\n","    # labels = miRNA_df['label'].values\n","\n","    # Create dictionary\n","    miRNA_dic = {\n","        RBP_name: {\n","            'miRNA_Sequence': padded_sequences,\n","            'miRNA_length': lengths\n","        }\n","    }\n","\n","    # Return the dictionary and other relevant information\n","    return miRNA_dic, max_length, tokenizer\n","miRNA_sequences_dic, max_length, tokenizer = process_miRNA_sequences(miRNA_path,\"AGO\",101)\n","miRNA_sequences_dic['AGO']['miRNA_length'][0]\n","\n","\n","def get_embeddings_and_probabilities_mirna(sequences, model, embedding_model):\n","    probabilities = model.predict(sequences).flatten()\n","    embeddings = embedding_model.predict(sequences)\n","\n","    # Instead of returning a dictionary with generic keys, use 'AGO' as the key for your data\n","    miRNA_embedding_dict = {\n","        'AGO': {\n","            \"embeddings\": embeddings,\n","            \"probabilities\": probabilities\n","        }\n","    }\n","\n","    return miRNA_embedding_dict\n","\n","# Assuming miRNA_sequences, miRNA_labels, miRNA_model, and miRNA_embedding_model are already defined\n","embedding_miRNA_dict = get_embeddings_and_probabilities_mirna(miRNA_sequences_dic['AGO']['miRNA_Sequence'], rna_model, embedding_model)\n","\n","embedding_miRNA_dict['AGO']['embeddings'].shape\n","\n","import numpy as np\n","\n","def sum_embeddings_mirna(embedding_dict, miRNA_dict):\n","    # Initialize a new dictionary to store the results\n","    summed_embeddings_dict = {}\n","\n","    # Iterate over each key in the input dictionary\n","    for key in embedding_dict.keys():\n","        print(f\"Processing Protein: {key}\")\n","        embeddings = embedding_dict[key]['embeddings']\n","\n","        # Get the corresponding miRNA lengths\n","        miRNA_lengths = miRNA_dict[key]['miRNA_length']\n","\n","        # Initialize a list to store summed embeddings for this key\n","        summed_embeddings_list = []\n","\n","        # Sum embeddings for each sequence using the correct length\n","        for i, miRNA_length in enumerate(miRNA_lengths):\n","            summed_embedding = np.sum(embeddings[i, :miRNA_length, :], axis=0)\n","            summed_embeddings_list.append(summed_embedding)\n","\n","        # Convert the list to a numpy array\n","        summed_embeddings = np.array(summed_embeddings_list)\n","\n","        # Save the summed embeddings in the new dictionary\n","        summed_embeddings_dict[key] = {\n","            'summed_embeddings': summed_embeddings\n","        }\n","\n","        # Print the shape to verify\n","        print(f\"Summed Embeddings Shape: {summed_embeddings.shape}\")\n","\n","    return summed_embeddings_dict\n","\n","\n","# import numpy as np\n","\n","# def sum_embeddings_mirna(embedding_dict, miRNA_dict):\n","#     # Initialize a new dictionary to store the results\n","#     summed_embeddings_dict = {}\n","#     i=0\n","\n","#     # Iterate over each key in the input dictionary\n","#     for key, value in embedding_dict.items():\n","#         print(f\"Processing Protein: {key}\")\n","#         embeddings = value['embeddings']\n","\n","#         # Get the corresponding miRNA lengths\n","#         miRNA_lengths = miRNA_dict[key]['miRNA_length']\n","# #\n","#         # Initialize a list to store summed embeddings for this key\n","#         summed_embeddings_list = []\n","#         miRNA_length = miRNA_lengths[i]\n","#         print(f\"miRNA_length: {miRNA_length}\")\n","#         # Sum embeddings along axis 1\n","#         summed_embeddings = np.sum(embeddings[:, :miRNA_length, :], axis=1)\n","\n","#         # Save the summed embeddings in the new dictionary\n","#         summed_embeddings_dict[key] = {\n","#             'summed_embeddings': summed_embeddings\n","#         }\n","\n","#         # Print the shape to verify\n","#         print(f\"Summed Embeddings Shape: {summed_embeddings.shape}\")\n","#         i+=1\n","\n","#     return summed_embeddings_dict\n","\n","embeddings_dict_miRNA = sum_embeddings_mirna(embedding_miRNA_dict,miRNA_sequences_dic)\n","\n","for key, value in embeddings_dict_miRNA.items():\n","    print(f\"Protein: {key}\")\n","    embeddings = value['summed_embeddings']\n","    print(f\"Embeddings Shape: {embeddings.shape}\")\n","    print(f\"First Embedding Sample: {embeddings[1]}\")"],"metadata":{"id":"KsfYJxzH72pI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import normalize\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","def flatten_embeddings(embeddings):ab, how I can conver my code ther  to .py file and\n","    # Normalize embeddings to unit length\n","    return normalize(embeddings, axis=1, norm='l2') #l1\n","\n","def calculate_average_similarity(embedding_rna_samples_dict, embedding_miRNA_dict):\n","    protein_similarity_dict = {}\n","\n","    # Extract and normalize the AGO miRNA embeddings\n","    miRNA_embeddings = embedding_miRNA_dict['AGO']['embeddings']\n","    # miRNA_embeddings = embedding_miRNA_dict['AGO']['summed_embeddings']\n","    miRNA_embeddings_flat = flatten_embeddings(miRNA_embeddings)\n","\n","    for protein, rna_data in embedding_rna_samples_dict.items():\n","        rna_embeddings = rna_data['embeddings']\n","        # rna_embeddings = rna_data['summed_embeddings']\n","        rna_embeddings_flat = flatten_embeddings(rna_embeddings)\n","\n","        # Calculate the cosine similarity matrix\n","        similarity_matrix = cosine_similarity(rna_embeddings_flat, miRNA_embeddings_flat)\n","\n","        # Calculate the average similarity for the entire matrix\n","        average_similarity = np.mean(similarity_matrix)\n","\n","        # Store the average similarity score for each protein\n","        protein_similarity_dict[protein] = average_similarity\n","\n","    # Sort the dictionary by similarity score in descending order\n","    sorted_protein_average_similarity_dict = dict(sorted(protein_similarity_dict.items(), key=lambda item: item[1], reverse=True))\n","\n","    return sorted_protein_average_similarity_dict\n","\n","# Assuming that embedding_rna_samples_dict and embedding_miRNA_dict are properly defined and populated\n","#Attention\n","protein_average_similarity_scores = calculate_average_similarity(embedding_rna_samples_dict, embedding_miRNA_dict)\n","#bi-directional\n","# protein_average_similarity_scores = calculate_average_similarity(summed_embeddings_dict, embeddings_dict_miRNA)\n","protein_average_similarity_scores\n","\n","\n","def flatten_embeddings(embeddings):\n","    # Ensures the output is 2D: (num_samples, 1)\n","    return np.mean(embeddings, axis=1).reshape(-1, 1)\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","def calculate_average_similarity(embedding_rna_samples_dict, embedding_miRNA_dict):\n","    protein_similarity_dict = {}\n","\n","    # Extract and flatten the AGO miRNA embeddings\n","    miRNA_embeddings = embedding_miRNA_dict['AGO']['embeddings']\n","    # miRNA_embeddings = embedding_miRNA_dict['AGO']['summed_embeddings']\n","    miRNA_embeddings_flat = flatten_embeddings(miRNA_embeddings)\n","\n","    for protein, rna_data in embedding_rna_samples_dict.items():\n","        rna_embeddings = rna_data['embeddings']\n","        # rna_embeddings = rna_data['summed_embeddings']\n","        # Flatten the RNA embeddings\n","        rna_embeddings_flat = flatten_embeddings(rna_embeddings)\n","\n","        # Calculate the cosine similarity matrix\n","        similarity_matrix = cosine_similarity(rna_embeddings_flat, miRNA_embeddings_flat)\n","\n","        # Calculate the average similarity for the entire matrix\n","        average_similarity = np.mean(similarity_matrix)\n","\n","        # Store the average similarity score for each protein\n","        protein_similarity_dict[protein] = average_similarity\n","\n","    # Sort the dictionary by similarity score in descending order\n","    sorted_protein_average_similarity_dict = dict(sorted(protein_similarity_dict.items(), key=lambda item: item[1], reverse=True))\n","\n","    return sorted_protein_average_similarity_dict\n","\n","\n","#Attention layer\n","protein_average_similarity_scores = calculate_average_similarity(embedding_rna_samples_dict, embedding_miRNA_dict)\n","#bi-Direction layer\n","# protein_average_similarity_scores = calculate_average_similarity(summed_embeddings_dict, embeddings_dict_miRNA)\n","protein_average_similarity_scores\n","\n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","def flatten_embeddings(embeddings):\n","    # Averages embeddings across the sequence length dimension\n","    # This transforms the shape from (num_samples, sequence_length, embedding_dim)\n","    # to (num_samples, embedding_dim)\n","    return np.mean(embeddings, axis=1)\n","\n","def calculate_average_similarity(embedding_rna_samples_dict, embedding_miRNA_dict):\n","    protein_similarity_dict = {}\n","\n","    # Assuming 'AGO' miRNA embeddings as the comparison base\n","    miRNA_embeddings = embedding_miRNA_dict['AGO']['embeddings']\n","    # Flatten the miRNA embeddings\n","    miRNA_embeddings_flat = flatten_embeddings(miRNA_embeddings)\n","\n","    for protein, rna_data in embedding_rna_samples_dict.items():\n","        rna_embeddings = rna_data['embeddings']\n","        # Flatten the RNA embeddings\n","        rna_embeddings_flat = flatten_embeddings(rna_embeddings)\n","\n","        # Calculate the cosine similarity matrix between flattened embeddings\n","        similarity_matrix = cosine_similarity(rna_embeddings_flat, miRNA_embeddings_flat)\n","\n","        # Calculate the average similarity for the entire matrix\n","        average_similarity = np.mean(similarity_matrix)\n","\n","        # Store the average similarity score for the protein\n","        protein_similarity_dict[protein] = average_similarity\n","\n","        # Sort the dictionary by similarity in descending order\n","        sorted_protein_avrage__similarity_dict = dict(sorted(protein_similarity_dict.items(), key=lambda item: item[1], reverse=True))\n","\n","    return sorted_protein_avrage__similarity_dict\n","protein_average_similarity_scores = calculate_average_similarity(embedding_rna_samples_dict, embedding_miRNA_dict)\n","protein_average_similarity_scores\n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","def flatten_embeddings(embeddings):\n","    \"\"\"\n","    Averages embeddings across the sequence length dimension, converting from\n","    3D shape (num_samples, sequence_length, embedding_dim) to 2D (num_samples, embedding_dim).\n","    \"\"\"\n","    return np.mean(embeddings, axis=1)\n","\n","def calculate_max_similarity(embedding_rna_samples_dict, embedding_miRNA_dict):\n","    protein_similarity_dict = {}\n","\n","    # We are only comparing with 'AGO' miRNA embeddings in the current setup\n","    miRNA_embeddings = embedding_miRNA_dict['AGO']['embeddings']\n","    # Flatten the miRNA embeddings\n","    miRNA_embeddings_flat = flatten_embeddings(miRNA_embeddings)\n","\n","    for protein, rna_data in embedding_rna_samples_dict.items():\n","        rna_embeddings = rna_data['embeddings']\n","        # Flatten the RNA embeddings\n","        rna_embeddings_flat = flatten_embeddings(rna_embeddings)\n","\n","        # Calculate the cosine similarity matrix between flattened embeddings\n","        similarity_matrix = cosine_similarity(rna_embeddings_flat, miRNA_embeddings_flat)\n","\n","        # Calculate the maximum similarity for the entire matrix\n","        max_similarity = np.max(similarity_matrix)\n","\n","        # Store the maximum similarity score for the protein\n","        protein_similarity_dict[protein] = max_similarity\n","\n","    # Sort the dictionary by similarity in descending order\n","    sorted_protein_similarity_dict = dict(sorted(protein_similarity_dict.items(), key=lambda item: item[1], reverse=True))\n","\n","    return sorted_protein_similarity_dict\n","protein_max_similarity_scores = calculate_max_similarity(embedding_rna_samples_dict, embedding_miRNA_dict)\n","protein_max_similarity_scores"],"metadata":{"id":"L8fR0aQH8SJz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from keras.models import Model\n","from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Concatenate\n","from keras.optimizers import Adam\n","from keras import backend as K\n","\n","# Define a custom layer for cosine similarity\n","class CosineSimilarityLayer(Layer):\n","    def __init__(self, **kwargs):\n","        super(CosineSimilarityLayer, self).__init__(**kwargs)\n","\n","    def call(self, inputs):\n","        x, y = inputs\n","        x = K.l2_normalize(x, axis=-1)\n","        y = K.l2_normalize(y, axis=-1)\n","        return K.batch_dot(x, y, axes=-1)\n","\n","def create_cnn_model(input_shape, conv_filters=(64, 128), kernel_sizes=(3, 3), pool_size=2, dense_units=256, dropout_rate=0.5, learning_rate=0.0001):\n","    # Input layers for PSSM and PSCM\n","    pssm_input = Input(shape=input_shape, name='pssm_input')\n","    pscm_input = Input(shape=input_shape, name='pscm_input')\n","\n","    # Convolutional layers for PSSM\n","    x_pssm = Conv2D(conv_filters[0], kernel_size=kernel_sizes, activation='relu', padding='same')(pssm_input)\n","    x_pssm = MaxPooling2D(pool_size=(pool_size, pool_size))(x_pssm)\n","    x_pssm = Conv2D(conv_filters[1], kernel_size=kernel_sizes, activation='relu', padding='same')(x_pssm)\n","    x_pssm = MaxPooling2D(pool_size=(pool_size, pool_size))(x_pssm)\n","    x_pssm = Flatten()(x_pssm)\n","    x_pssm = Dense(dense_units, activation='relu')(x_pssm)\n","    x_pssm = Dropout(dropout_rate)(x_pssm)\n","\n","    # Convolutional layers for PSCM\n","    x_pscm = Conv2D(conv_filters[0], kernel_size=kernel_sizes, activation='relu', padding='same')(pscm_input)\n","    x_pscm = MaxPooling2D(pool_size=(pool_size, pool_size))(x_pscm)\n","    x_pscm = Conv2D(conv_filters[1], kernel_size=kernel_sizes, activation='relu', padding='same')(x_pscm)\n","    x_pscm = MaxPooling2D(pool_size=(pool_size, pool_size))(x_pscm)\n","    x_pscm = Flatten()(x_pscm)\n","    x_pscm = Dense(dense_units, activation='relu')(x_pscm)\n","    x_pscm = Dropout(dropout_rate)(x_pscm)\n","\n","    # Concatenate PSSM and PSCM encoded representations\n","    concatenated = Concatenate()([x_pssm, x_pscm])\n","\n","    # Dense and dropout layers\n","    x = Dense(dense_units, activation='relu')(concatenated)\n","    x = Dropout(dropout_rate)(x)\n","\n","    # Output layer for similarity score\n","    similarity_score = Dense(1, activation='sigmoid')(x)\n","\n","    # Model for training\n","    model = Model(inputs=[pssm_input, pscm_input], outputs=similarity_score)\n","    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])\n","\n","    # Define a function to get embeddings\n","    embedding_model = Model(inputs=[pssm_input, pscm_input], outputs=concatenated)\n","\n","    return model, embedding_model\n","\n","# Example usage\n","input_shape = (100, 100, 1)  # Example input shape, should be adapted to your data\n","model, embedding_model = create_cnn_model(input_shape)\n","model.summary()\n"],"metadata":{"id":"pHD_PZdw8nm-"},"execution_count":null,"outputs":[]}]}